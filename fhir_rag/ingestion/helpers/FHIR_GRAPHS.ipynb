{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a6f735197fafac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RAG on FHIR with Knowledge Graphs\n",
    "\n",
    "This notebook, and associated Python files, covers loading FHIR resources into a Graph Database (in this case [Neo4j](https://www.neo4j.com)), and then using the resulting graph for Retrieval Augmented Generation (RAG). RAG is a process where contextual information is retrieved and used to augment a request to an LLM. To learn more about the basics of RAG you can view my [YouTube video](https://youtu.be/2XVYQeWbuz4) or read my [Medium article](https://medium.com/@samschifman/rag-on-fhir-29a9771f49b6). \n",
    "\n",
    "\n",
    "## Disclaimer\n",
    "Nothing provided here is guaranteed or warrantied to work. It is provided as is and has not been tested extensively. Using this notebook is at the risk of the user. \n",
    "\n",
    "\n",
    "## Prerequisites & Setup\n",
    "This notebook assumes a number of things:\n",
    "\n",
    "\n",
    "### 1. Ollama\n",
    "This notebook uses [Ollama](https://ollama.ai/) to run LLM models locally. It could be modified to use OpenAI or any other LLM supported by LangChain, but this is not covered here. To use this notebook as is, you will need ot install Ollama. \n",
    "\n",
    "\n",
    "### 2. Neo4J & Jupyter Environment\n",
    "This notebook needs an instance of [Neo4j](https://www.neo4j.com) to talk to. I used docker to run Neo4J locally using the following command:\n",
    "```\n",
    "docker run --name testneo4j -p7474:7474 -p7687:7687 -d \\\n",
    "    -v $HOME/neo4j/data:/data \\\n",
    "    -v $HOME/neo4j/logs:/logs \\\n",
    "    -v $HOME/neo4j/import:/var/lib/neo4j/import \\\n",
    "    -v $HOME/neo4j/plugins:/plugins \\\n",
    "    --env NEO4J_AUTH=neo4j/password \\\n",
    "    neo4j:latest\n",
    "```\n",
    "**Note:** No particular plugins are needed. \n",
    "\n",
    "You can also use a Neo4J Aurora instance. \n",
    "\n",
    "#### Jupyter Environment\n",
    "Regardless of how you run Neo4J. You need to set some environment variables in the notebook's environment:\n",
    "\n",
    "| Variable | Description | Value for above Docker |\n",
    "|----------|-------------|------------------------|\n",
    "| NEO4J_URL | Where to find the instance of Neo4j. | bolt://localhost:7687 |\n",
    "| NEO4J_USER | The username for the database. | neo4j |\n",
    "| NEO4J_PASSWORD | The password for the database. | password |\n",
    "\n",
    "\n",
    "### 3. Synthetic data and working directory\n",
    "The data I used for this notebook came from [Synthea](https://synthea.mitre.org/). In theory, you should be able to use any FHIR bundle, but it was only tested with Synthia data. In particular, I used the pre-generated data available [here](https://github.com/synthetichealth/synthea-sample-data/blob/1fe1beaa80a8fbe7b64c0c135bcbb8b1346ef38a/downloads/latest/synthea_sample_data_fhir_latest.zip). \n",
    "\n",
    "All the questions here us the FHIR Bundle: `Alfonso758_Bins636_e80d4c62-149a-a6a6-4b39-9d4aa3e07ba7.json`\n",
    "\n",
    "The notebook further assumes that you have setup a working directory (called `working`) at the same level as the notebook. Inside this working directory you need to create a subdirectory called `bundles` and put the bundles you want loaded into the graph in there. \n",
    "\n",
    "I have it setup as:\n",
    "```\n",
    "| - FHIR_GRAPH.ipynb\n",
    "| - FHIR_flattener.py\n",
    "| - FHIR_to_string.py\n",
    "| - NEO4J_Graph.py\n",
    "| - working\\\n",
    "- - | - bundles\\\n",
    "- - - - | - Alfonso758_Bins636_e80d4c62-149a-a6a6-4b39-9d4aa3e07ba7.json\n",
    "- - - - | - hospitalInformation1701791555719.json\n",
    "- - - - | - practitionerInformation1701791555719.json\n",
    "```\n",
    "\n",
    "## Special Thanks To\n",
    "Much of this notebook is inspired by the [Neo4J Going Meta talks](https://github.com/jbarrasa/goingmeta/tree/main). In particular [Session 23: Advanced RAG patterns with Knowledge Graphs](https://www.youtube.com/watch?v=E_JO4-2D5Xs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a207f721a36a2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install some packages that are needed. \n",
    "\n",
    "!pip install sentence_transformers neo4j langchain pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Imports from other local python files\n",
    "from NEO4J_Graph import Graph\n",
    "from FHIR_to_graph import resource_to_node, resource_to_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488878855fedfc7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Establish Database Connection\n",
    "\n",
    "The cell connects to the Neo4J instance. It relies on several environment variables. \n",
    "\n",
    "**PLEASE NOTE**: The variable have been changed to support multiple databases in the same instance. \n",
    "\n",
    "| Variable            | Description                          | Sample Value          |\n",
    "|---------------------|--------------------------------------|-----------------------|\n",
    "| FHIR_GRAPH_URL      | Where to find the instance of Neo4j. | bolt://localhost:7687 |\n",
    "| FHIR_GRAPH_USER     | The username for the database.       | neo4j                 |\n",
    "| FHIR_GRAPH_PASSWORD | The password for the database.       | password              |\n",
    "| FHIR_GRAPH_DATABASE | The name of the database instance.   | neo4j                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fc5079bb6751e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NEO4J_URI = os.getenv('FHIR_GRAPH_URL')\n",
    "USERNAME = os.getenv('FHIR_GRAPH_USER')\n",
    "PASSWORD = os.getenv('FHIR_GRAPH_PASSWORD')\n",
    "DATABASE = os.getenv('FHIR_GRAPH_DATABASE')\n",
    "\n",
    "graph = Graph(NEO4J_URI, USERNAME, PASSWORD, DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85fd713e8934a2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Helper Database Cells\n",
    "\n",
    "The following three cells are here to be used to manage the database. They do not need to be run on a blank database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdcff60ba4d2337",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(graph.resource_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1705bae54c296",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(graph.database_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2709b11c2895a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph.wipe_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9387a628b270ec3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load FHIR into the Graph\n",
    "\n",
    "This cell opens the bundle and creates the nodes and edges in the graph for each resource. \n",
    "\n",
    "Every resource will result in a node that has a label based on the resource type and as a `resource`. The values within the resource will be flattened \n",
    "into properties within the node. Also, a property called `text` will include a string representation of the resource. \n",
    "\n",
    "Additionally, nodes will be created for every unique date (ignoring time) found in the FHIR resources. \n",
    "\n",
    "Edges will be created for every reference in the resource to something that can be found within the bundles loaded. So the linking resource doesn't have \n",
    "to be in the same bundle, but it must be in a bundle that is loaded. \n",
    "\n",
    "Edges will also connect resources to the dates found inside them. \n",
    "\n",
    "**Warning:** This cell may take sometime to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839cca2360b0feb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "synthea_bundles = glob.glob(\"./working/bundles/*.json\")\n",
    "synthea_bundles.sort()\n",
    "\n",
    "nodes = []\n",
    "edges = []\n",
    "dates = set() # set is used here to make sure dates are unique\n",
    "for bundle_file_name in synthea_bundles:\n",
    "    with open(bundle_file_name) as raw:\n",
    "        bundle = json.load(raw)\n",
    "        for entry in bundle['entry']:\n",
    "            resource_type = entry['resource']['resourceType']\n",
    "            if resource_type != 'Provenance':\n",
    "                # generated the cypher for creating the resource node \n",
    "                nodes.append(resource_to_node(entry['resource']))\n",
    "                # generated the cypher for creating the reference & date edges and capture dates\n",
    "                node_edges, node_dates = resource_to_edges(entry['resource'])\n",
    "                edges += node_edges\n",
    "                dates.update(node_dates)\n",
    "\n",
    "# create the nodes for resources\n",
    "for node in nodes:\n",
    "    graph.query(node)\n",
    "\n",
    "\n",
    "date_pattern = re.compile(r'([0-9]+)/([0-9]+)/([0-9]+)')\n",
    "\n",
    "# create the nodes for dates\n",
    "for date in dates:\n",
    "    date_parts = date_pattern.findall(date)[0]\n",
    "    cypher_date = f'{date_parts[2]}-{date_parts[0]}-{date_parts[1]}'\n",
    "    cypher = 'CREATE (:Date {name:\"' + date + '\", id: \"' + date + '\", date: date(\"' + cypher_date + '\")})'\n",
    "    graph.query(cypher)\n",
    "\n",
    "# create the edges\n",
    "for edge in edges:\n",
    "    try:\n",
    "        graph.query(edge)\n",
    "    except:\n",
    "        print(f'Failed to create edge: {edge}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce02a82986684bc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out some information to show that the graph is populated.\n",
    "print(graph.resource_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13005f12fd04187d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create the Vector Embedding Index in the Graph\n",
    "\n",
    "This cell creates a Vector Index in Neo4J. It looks at nodes labeled as `resource` and indexes the string representation in the `text` property. \n",
    "\n",
    "**Warning:** This cell may take sometime to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1671e2ac55b0a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Neo4jVector.from_existing_graph(\n",
    "    HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    url=NEO4J_URI,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    database=DATABASE,\n",
    "    index_name='fhir_text',\n",
    "    node_label=\"resource\",\n",
    "    text_node_properties=['text'],\n",
    "    embedding_node_property='embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e19170b259b1ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Vector Index \n",
    "\n",
    "This cell creates a new vector index, using the index created above. \n",
    "\n",
    "This is here because running the cell above can take time and only should be done one time when the DB is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc331fc7da8f322",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_index(\n",
    "    HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    url=NEO4J_URI,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    database=DATABASE,\n",
    "    index_name='fhir_text'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1dc5bb3d954d7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup Prompt Templates\n",
    "\n",
    "This cell sets the prompt template to use when calling the LLM. I have been experimenting with multiple forms of the prompt to improve \n",
    "the results from the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ba110d9233f8b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default_prompt='''\n",
    "System: Use the following pieces of context to answer the user's question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "Human: {question}\n",
    "'''\n",
    "\n",
    "my_prompt='''\n",
    "System: The following information contains entries about the patient. \n",
    "Use the primary entry and then the secondary entries to answer the user's question.\n",
    "Each entry is its own type of data and secondary entries are supporting data for the primary one. \n",
    "You should restrict your answer to using the information in the entries provided. \n",
    "\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "----------------\n",
    "User: {question}\n",
    "'''\n",
    "\n",
    "my_prompt_2='''\n",
    "System: The context below contains entries about the patient's healthcare. \n",
    "Please limit your answer to the information provided in the context. Do not make up facts. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "If you are asked about the patient's name and one the entries is of type patient, you should look for the first given name and family name and answer with: [given] [family]\n",
    "----------------\n",
    "{context}\n",
    "Human: {question}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(my_prompt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52e3eccdb650b1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pick the LLM model to use\n",
    "\n",
    "Ollama can run multiple models. I had the most luck with mistral. However, you could try others. The list of possible \n",
    "models is [here](https://ollama.ai/library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d24f18813691e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ollama_model = 'mistral' # mistral, orca-mini, llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b416987d38353",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Set K Nearest\n",
    "\n",
    "This is the number of nearest neighbors to find in our similarity search. In most cases, the result will be limited to the top one in the retrieval query, but we need this number to be large because there can be a large number of resources of the same type. For example, when searching for Explanation of Benefits there are 125 possible ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb0840f5de42d0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_nearest = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a369e0293a1944",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pick the Question\n",
    "\n",
    "All following cells will work with the question as defined here. As you can see, I have been experimenting with a number of \n",
    "different questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d405ebfa31fa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# question = \"What can you tell me about Alfonso's claim created on 03/06/1977?\"\n",
    "# question = \"What can you tell me about the medical claim created on 03/06/1977?\"\n",
    "# question = \"Based on this explanation of benefits, how much did it cost and what service was provided?\"\n",
    "# question = \"Based on this explanation of benefits created on July 15, 2016, how much did it cost and what service was provided?\"\n",
    "# question = \"Based on this explanation of benefits created on March 6, 1978, how much did it cost and what service was provided?\"\n",
    "# question = \"Based on this explanation of benefits created on January 11, 2009, how much did it cost and what service was provided?\"\n",
    "# question = \"What was the blood pressure on 2/9/2014?\"\n",
    "# question = \"What was the blood pressure?\"\n",
    "# question = \"Based on this explanation of benefits created on January 18, 2014, how much did it cost and what service was provided?\"\n",
    "# question = \"How much did the colon scan eighteen days after the first of the year 2019 cost?\"\n",
    "question = \"How much did the colon scan on Jan. 18, 2014 cost?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbfb8afd117973",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Start working with the LLM\n",
    "\n",
    "The rest of this notebook is working with the LLM to attempt to answer the question.\n",
    "\n",
    "### Ask LLM\n",
    "\n",
    "This first cell asks the LLM with no context and gets told the LLM can't answer without more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0c7d377ec2878",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llm = Ollama(model=ollama_model)\n",
    "no_rag_answer = llm(question)\n",
    "print(no_rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66084a4b344135ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check Vector Index\n",
    "\n",
    "This cell checks what the vector index will return and is here for debugging / informational purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2f162722c00d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = vector_index.similarity_search(question, k=1) # k_nearest is not used here because we don't have a retrieval query yet.\n",
    "print(response[0].page_content)\n",
    "print(len(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e24b19bda506b2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ask the LMM with Context\n",
    "\n",
    "This cell will ask the LLM with the string representation of the resource node that is found by the vector index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370d5987305ba74",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOllama(model=ollama_model), chain_type=\"stuff\", retriever=vector_index.as_retriever(search_kwargs={'k': 1}), # k_nearest is not used here because we don't have a retrieval query yet.\n",
    "    verbose=True, chain_type_kwargs={\"verbose\": True, \"prompt\": prompt}\n",
    ")\n",
    "\n",
    "pprint(vector_qa.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8707ec97ad70c81",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Vector Index with Enhanced Context\n",
    "\n",
    "This cell creates a new vector index, reusing the index created above, that also enhances the results with neighboring nodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5069ffdfd5fd80d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contextualize_query = \"\"\"\n",
    "match (node)<-[]->(sc:resource)\n",
    "with node.text as self, reduce(s=\"\", item in collect(distinct sc.text) | s + \"\\n\\nSecondary Entry:\\n\" + item ) as ctxt, score, {} as metadata limit 1\n",
    "return \"Primary Entry:\\n\" + self + ctxt as text, score, metadata\n",
    "\"\"\"\n",
    "\n",
    "contextualized_vectorstore = Neo4jVector.from_existing_index(\n",
    "    HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    url=NEO4J_URI,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    database=DATABASE,\n",
    "    index_name='fhir_text',\n",
    "    retrieval_query=contextualize_query,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb7a232a9181ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check Vector Index with Enhanced Context\n",
    "\n",
    "This cell checks what the vector index will return and is here for debugging / informational purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bbdcefa6807a0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = contextualized_vectorstore.similarity_search(question, k=k_nearest)\n",
    "\n",
    "print(response[0].page_content)\n",
    "print(len(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d2e3c8108f063",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ask the LLM with Enhanced Context\n",
    "\n",
    "This cell uses a Cypher query to expand the context to include cells connected to the node returned by the vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f23cedfc76ce25",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOllama(model=ollama_model), chain_type=\"stuff\", \n",
    "    retriever=contextualized_vectorstore.as_retriever(search_kwargs={'k': k_nearest}), \n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\"verbose\": True, \"prompt\": prompt}\n",
    ")\n",
    "\n",
    "pprint(vector_qa.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d950d97af0f757",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Include Respect for Dates \n",
    "\n",
    "Up until now we have only been looking at vector index that looks at the `text` property. However, this does not do a good job of respecting dates \n",
    "inside the question. From here on we will include respecting those dates. \n",
    "\n",
    "### Find the Pertinent Date  \n",
    "\n",
    "This call looks in the question and uses the LLM to extract the date from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ef887a253ba8d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def date_for_question(question_to_find_date, model):\n",
    "    _llm = Ollama(model=model) \n",
    "    _response = _llm(f'''\n",
    "    system:Given the following question from the user, extract the date the question is asking about.\n",
    "    Return the answer formatted as JSON only, as a single line.\n",
    "    Use the form:\n",
    "    \n",
    "    {{\"date\":\"[THE DATE IN THE QUESTION]\"}}\n",
    "    \n",
    "    Use the date format of month/day/year.\n",
    "    Use two digits for the month and day.\n",
    "    Use four digits for the year.\n",
    "    So 3/4/23 should be returned as {{\"date\":\"03/04/2023\"}}.\n",
    "    So 04/14/89 should be returned as {{\"date\":\"04/14/1989\"}}.\n",
    "    \n",
    "    Please do not include any special formatting characters, like new lines or \"\\\\n\".\n",
    "    Please do not include the word \"json\".\n",
    "    Please do not include triple quotes.\n",
    "    \n",
    "    If there is no date, do not make one up. \n",
    "    If there is no date return the word \"none\", like: {{\"date\":\"none\"}}\n",
    "    \n",
    "    user:{question_to_find_date}\n",
    "    ''')\n",
    "    date_json = json.loads(_response)\n",
    "    return date_json['date']\n",
    "\n",
    "date_str = date_for_question(question, ollama_model)\n",
    "print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dffb0d916fde2b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Vector Index with Date Aware Enhanced Context\n",
    "\n",
    "In this cell we add a restriction in vector index to make sure the returned node is associated with the date in the question. \n",
    "\n",
    "**Warning:** This has several limitation:\n",
    "* It does not gracefully handle the case where the question doesn't have a date. It just falls back on the behavior above. \n",
    "* It does not handle if there are multiple dates in the question.\n",
    "* It does not handle if the question implies a range, such as \"all encounters before June 1, 2010.\"\n",
    "* It does not work if the node in question isn't with the 10 closest nodes to question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb78de80af5c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_contextualized_vectorstore_with_date(date_to_look_for):\n",
    "    if date_to_look_for == 'none':\n",
    "        contextualize_query_with_date = \"\"\"\n",
    "        match (node)<-[]->(sc:resource)\n",
    "        with node.text as self, reduce(s=\"\", item in collect(distinct sc.text) | s + \"\\n\\nSecondary Entry:\\n\" + item ) as ctxt, score, {} as metadata limit 1\n",
    "        return \"Primary Entry:\\n\" + self + ctxt as text, score, metadata\n",
    "        \"\"\"\n",
    "    else:\n",
    "        contextualize_query_with_date = f\"\"\"\n",
    "        match (node)<-[]->(sc:resource)\n",
    "        where exists {{\n",
    "             (node)-[]->(d:Date {{id: '{date_to_look_for}'}})\n",
    "        }}\n",
    "        with node.text as self, reduce(s=\"\", item in collect(distinct sc.text) | s + \"\\n\\nSecondary Entry:\\n\" + item ) as ctxt, score, {{}} as metadata limit 1\n",
    "        return \"Primary Entry:\\n\" + self + ctxt as text, score, metadata\n",
    "        \"\"\"\n",
    "    \n",
    "    _contextualized_vectorstore_with_date = Neo4jVector.from_existing_index(\n",
    "        HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "        url=NEO4J_URI,\n",
    "        username=USERNAME,\n",
    "        password=PASSWORD,\n",
    "        database=DATABASE,\n",
    "        index_name='fhir_text',\n",
    "        retrieval_query=contextualize_query_with_date,\n",
    "    )\n",
    "    return _contextualized_vectorstore_with_date\n",
    "\n",
    "contextualized_vectorstore_with_date = create_contextualized_vectorstore_with_date(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28595c9b2723c2e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check Vector Index with Date Aware Enhanced Context\n",
    "\n",
    "This cell checks what the vector index will return and is here for debugging / informational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590cf26919bb510",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = contextualized_vectorstore_with_date.similarity_search(question, k=k_nearest)\n",
    "\n",
    "print(response[0].page_content)\n",
    "print(len(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5a7ac2ce5f87",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ask the LLM with Date Aware Enhanced Context\n",
    "\n",
    "This cell uses a Cypher query to expand the context to include cells connected to the node returned by the vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3328a2760dd0da",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOllama(model=ollama_model), chain_type=\"stuff\",\n",
    "    retriever=contextualized_vectorstore_with_date.as_retriever(search_kwargs={'k': k_nearest}),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\"verbose\": True, \"prompt\": prompt}\n",
    ")\n",
    "\n",
    "print(vector_qa.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344997ba61265449",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bring it Together \n",
    "\n",
    "This cell brings it together into a single method that answers questions with or without dates in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18f7e05ee415d8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ask_date_question(question_to_ask, model=ollama_model, prompt_to_use=prompt):\n",
    "    _date_str = date_for_question(question_to_ask, model)\n",
    "    _index = create_contextualized_vectorstore_with_date(_date_str)\n",
    "    _vector_qa = RetrievalQA.from_chain_type(\n",
    "        llm=ChatOllama(model=model), chain_type=\"stuff\",\n",
    "        retriever=_index.as_retriever(search_kwargs={'k': k_nearest}),\n",
    "        verbose=True,\n",
    "        chain_type_kwargs={\"verbose\": True, \"prompt\": prompt_to_use}\n",
    "    )\n",
    "    return _vector_qa.run(question_to_ask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96b5bcde32e3c6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_date_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113e5c84e802742",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_date_question(\"What was the name of the patient whose respiratory rate was captured on 2/26/2017?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85e8b51c20546b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_date_question(\"Based on this explanation of benefits created on January 18, 2014, how much did it cost and what service was provided?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b914826ba2d861",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_date_question(\"How much did the colonoscopy on 1/18/14 cost?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8d15a73cbcc65",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_date_question(\"How much did the colon scan eighteen days after the first of the year 2014 cost?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1190aef9b922e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_date_question(\"How much did the colon scan on Jan. 18, 2014 cost?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7a8fcde5de95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Disclaimer:** Nothing provided here is guaranteed or warrantied to work. It is provided as is and has not been tested extensively. Using this notebook is at the risk of the user. \n",
    "\n",
    "Copyright &copy; 2024 Sam Schifman"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
