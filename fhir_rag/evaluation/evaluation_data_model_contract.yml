# Metadata for the experimentation process
experimentation_id: uuid
    # Unique identifier for each experimentation session.

experimentation_timestamp: timestamp
    # Timestamp marking when the experimentation was conducted.

experimentation_design: str 
    # Consider adding fields related to the experimental design, such as hypothesis, variables, and control groups.

experimentation_status: enum
    # possible values: ['in_progress', 'completed', 'failed']

# Consumer/Patient-related information
consumer_id: str
    # Unique identifier for the patient or FHIR resource bundle reference.

# Retrieval phase details
retrieval_phase:
    retrieval_strategy: enum
        # Retrieval technique used in the experimentation.
        # Possible values: ['lexical_search', 'lexical_search_graph_bfs', 'similarity_search', 'similarity_search_bfs'].
    
    configuration: json
        # JSON object containing configurations for the retrieval phase, such as:
        # - embedding_model: Model used for embedding.
        # - k: Number of retrieved neighbors.
        # - similarity_threshold: Minimum threshold for similarity.
        # Example: {"embedding_model": "BAAI/bge-small-en-v1.5", "k": 100, "similarity_threshold": 0.85}.

    retrieval_query: str
        # Query used to fetch the results in the GraphKB.

    retrieval_results: str
        # JSON object containing the retrieval results, such as the retrieved resources and their details.
        # Example: "{"Patient": {"resource_id": "12345", "resource_name": "John Doe"}, "Observation": {"resource_id": "67890", "resource_name": "Blood Pressure"}}".

    latency_seconds: double
        # Time in seconds to retrieve data from the knowledge graph (graphKB).

    tokens_count: int
        # Number of tokens in the output of the retrieval (base on the inference model "generation_phase.model_name").

    characters_count: int
        # Number of characters in the output of the retrieval.

    fhir_resources: json
        # JSON object detailing the FHIR resources retrieved, including resource type and count.
        # Example: {"Patient": 1, "Observation": 3, "Condition": 2}.

# Augmentation phase details
augmentation_phase:
    input_prompt: str
        # The user-input prompt or task for the LLM evaluation.

    task_type: enum
        # Type of NLP task being evaluated.
        # Possible values: ['summarization', 'information_extraction', 'question_answering', 'dialogue_generation'].

    system_prompt: str
        # System-level prompt used to guide the experimentation (includes instructions or rules for the LLM).

    ground_truth: str
        # Ground-truth answer or expected result, used as a baseline for evaluation.

    input_tokens_count: int
        # Number of tokens in the input prompt (base on the inference model "generation_phase.model_name").

    input_characters_count: int
        # Number of characters in the input prompt.

# Generation phase of the RAG
generation_phase:
    model_name: str
        # Name of the LLM used for generating responses.

    model_configuration: json
        # JSON object containing model configurations for the generation phase, such as:
        # - temperature: Value to control randomness in generation.
        # - max_tokens: Limit on tokens in the generated output.
        # - model_version: Specific version of the LLM used.

    output_text: str
        # The output generated by the LLM in response to the input prompt.

    output_latency_seconds: double
        # Time in seconds to generate the LLM output.

    output_tokens_count: int
        # Number of tokens in the LLM output.

    output_characters_count: int
        # Number of characters in the LLM output.

# Retrieval-Augmented Generation (RAG) phase details
rag_phase:
    rag_model_name: str
        # Name of the RAG (Retrieval-Augmented Generation) model used for evaluation.

    rag_configuration: json
        # Configuration details for the RAG phase, including:
        # - retrieval_depth: Depth of search in graph-based retrieval.
        # - context_window: Size of context passed to the LLM.

# RAGAs evaluation metrics
ragas_evaluation:
    evaluation_metrics: json
        # JSON object detailing evaluation metrics from the RAGAs framework, such as:
        # - precision: Measure of relevant retrieved resources.
        # - recall: Measure of successfully retrieved resources.
        # - f1_score: Balance between precision and recall.
        # - response_relevance_score: Relevance of the LLM output against ground truth.

    execution_latency_seconds: double
        # Time taken in seconds by the RAGAs framework to compute evaluation metrics.

# Agentic workflow metadata for tracking
agent_workflow:
    metadata: json
        # Additional metadata for agentic workflows, such as:
        # - agent_type: Type of agent involved (e.g., GraphRAG, system-driven).
        # - workflow_step: Specific step of the agentic workflow being executed.
        # - decision_reasoning: Reasoning for agent decisions at specific workflow steps.
        # Example: {"agent_type": "GraphRAG", "workflow_step": "retrieve_resources", "decision_reasoning": "high confidence"}.

    agent_performance: json
        # Metrics to track agent workflow performance, such as:
        # - agent_latency: Time taken for agents to make decisions.
        # - decision_accuracy: Accuracy of decisions made by the agents.

    agent_errors: json
        # Logs detailing errors or issues encountered by agents in the workflow.
        # Example: {"error_type": "data_unavailable", "step": "fetch_resources", "resolution": "retried"}.
